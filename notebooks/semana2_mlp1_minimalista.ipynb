{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e20f1b",
   "metadata": {},
   "source": [
    "# SEMANA 2 - MLP 1 MINIMALISTA\n",
    "\n",
    "**Arquitetura:** Input(15) ‚Üí Dense(16, ReLU) ‚Üí Output(1, Sigmoid)  \n",
    "**Par√¢metros:** 273 (rela√ß√£o 3.26 amostras/par√¢metro - SEGURO!)\n",
    "\n",
    "## Objetivos desta semana:\n",
    "1. ‚úÖ Carregar dados preprocessados (Semana 1)\n",
    "2. ‚úÖ Implementar arquitetura MLP 1\n",
    "3. ‚úÖ Definir hiperpar√¢metros (learning rate, batch size, epochs)\n",
    "4. ‚úÖ Treinar modelo com callbacks (EarlyStopping, ModelCheckpoint)\n",
    "5. ‚úÖ Avaliar performance (ROC-AUC, loss, curvas de aprendizado)\n",
    "6. ‚úÖ Salvar modelo e m√©tricas\n",
    "\n",
    "## Por que esta arquitetura?\n",
    "- **Simples e interpret√°vel** - apenas 1 camada oculta\n",
    "- **Baixo risco de overfitting** - 273 par√¢metros para 712 amostras de treino\n",
    "- **Baseline s√≥lido** - captura padr√µes principais sem decorar ru√≠dos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f91b86",
   "metadata": {},
   "source": [
    "## üìê ARQUITETURA VISUAL\n",
    "\n",
    "Veja como a informa√ß√£o flui pela rede neural:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    INPUT (15 features)                       ‚îÇ\n",
    "‚îÇ  [Sex, Age, Fare, HasCabin, Pclass_1, Pclass_2, ...]       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                            ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ              CAMADA OCULTA (16 neur√¥nios)                    ‚îÇ\n",
    "‚îÇ  [N1]  [N2]  [N3]  [N4]  ...  [N15]  [N16]                  ‚îÇ\n",
    "‚îÇ   ‚Üì     ‚Üì     ‚Üì     ‚Üì           ‚Üì      ‚Üì                    ‚îÇ\n",
    "‚îÇ  ReLU  ReLU  ReLU  ReLU  ...  ReLU   ReLU                   ‚îÇ\n",
    "‚îÇ                                                               ‚îÇ\n",
    "‚îÇ  ‚ö° DROPOUT 0.2 (desliga 20% aleatoriamente no treino)       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                            ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    OUTPUT (1 neur√¥nio)                       ‚îÇ\n",
    "‚îÇ                      Sigmoid                                 ‚îÇ\n",
    "‚îÇ                   (probabilidade 0-1)                        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Fluxo de dados:**\n",
    "1. **Entrada:** Dados de 1 passageiro (15 valores entre 0-1)\n",
    "2. **Camada Oculta:** Cada neur√¥nio \"olha\" todas as 15 features e aprende um padr√£o\n",
    "3. **ReLU:** Ativa o neur√¥nio se detectou algo importante (sen√£o = 0)\n",
    "4. **Dropout:** No treino, desliga 20% dos neur√¥nios aleatoriamente (evita decorar)\n",
    "5. **Sa√≠da:** Combina os 16 neur√¥nios ‚Üí probabilidade de sobreviver (0-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c262de93",
   "metadata": {},
   "source": [
    "## üßÆ C√ÅLCULO DE PAR√ÇMETROS (PASSO A PASSO)\n",
    "\n",
    "### CAMADA 1: Input (15) ‚Üí Hidden (16)\n",
    "\n",
    "**Para CADA neur√¥nio da camada oculta:**\n",
    "- 15 pesos (um conectando cada feature de entrada a este neur√¥nio)\n",
    "- 1 bias (valor de ajuste pr√≥prio do neur√¥nio)\n",
    "- **Total por neur√¥nio:** 15 + 1 = 16 par√¢metros\n",
    "\n",
    "**Para os 16 neur√¥nios:**\n",
    "- 16 neur√¥nios √ó 16 par√¢metros = **256 par√¢metros**\n",
    "\n",
    "**Exemplo visual - Neur√¥nio 1:**\n",
    "```\n",
    "Sex (0.7)    √ó peso_1  ‚Üí  \\\n",
    "Age (0.22)   √ó peso_2  ‚Üí   \\\n",
    "Fare (0.14)  √ó peso_3  ‚Üí    \\\n",
    "...                    ‚Üí     }‚Üí SOMA + bias_1 ‚Üí ReLU ‚Üí sa√≠da_neur√¥nio_1\n",
    "Pclass_1 (1) √ó peso_15 ‚Üí    /\n",
    "                           /\n",
    "                          /\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### CAMADA 2: Hidden (16) ‚Üí Output (1)\n",
    "\n",
    "**Para o neur√¥nio de sa√≠da:**\n",
    "- 16 pesos (um para cada neur√¥nio da camada oculta)\n",
    "- 1 bias\n",
    "- **Total:** 16 + 1 = **17 par√¢metros**\n",
    "\n",
    "---\n",
    "\n",
    "### TOTAL GERAL\n",
    "**256 + 17 = 273 par√¢metros trein√°veis**\n",
    "\n",
    "Com 712 amostras de treino:\n",
    "- **712 / 273 = 2.61 amostras por par√¢metro**\n",
    "- ‚úÖ **Margem segura!** Cada par√¢metro \"v√™\" ~3 exemplos diferentes\n",
    "- ‚ö†Ô∏è Se fosse 0.5 amostras/param = overfitting garantido!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d2315",
   "metadata": {},
   "source": [
    "## üì¶ Importar Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f39da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas essenciais\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.metrics import AUC, Precision, Recall\n",
    "\n",
    "# Visualiza√ß√£o e m√©tricas\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, classification_report, confusion_matrix\n",
    "\n",
    "# Configurar plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(f\"TensorFlow vers√£o: {tf.__version__}\")\n",
    "print(f\"Keras vers√£o: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43e6871",
   "metadata": {},
   "source": [
    "## üé≤ Fixar Seeds (Reprodutibilidade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4338eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Fixar seeds\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "print(\"‚úÖ Seeds fixadas! SEED =\", SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2429ad77",
   "metadata": {},
   "source": [
    "## üìÇ Carregar Dados (Semana 1)\n",
    "\n",
    "Carregamos os dados j√° separados e os √≠ndices salvos na Semana 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17d4976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar datasets separados\n",
    "X_train = pd.read_csv('../data/processed/X_train.csv')\n",
    "X_val = pd.read_csv('../data/processed/X_val.csv')\n",
    "y_train = pd.read_csv('../data/processed/y_train.csv').values.ravel()\n",
    "y_val = pd.read_csv('../data/processed/y_val.csv').values.ravel()\n",
    "\n",
    "# Carregar informa√ß√µes do split\n",
    "with open('../splits/split_indices.json', 'r') as f:\n",
    "    split_info = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Dados carregados:\")\n",
    "print(f\"   - X_train: {X_train.shape}\")\n",
    "print(f\"   - X_val: {X_val.shape}\")\n",
    "print(f\"   - y_train: {y_train.shape}\")\n",
    "print(f\"   - y_val: {y_val.shape}\")\n",
    "print(f\"\\nüìä Distribui√ß√£o (treino): Classe 0={split_info['train_class_distribution']['class_0']}, Classe 1={split_info['train_class_distribution']['class_1']}\")\n",
    "print(f\"üìä Distribui√ß√£o (val): Classe 0={split_info['val_class_distribution']['class_0']}, Classe 1={split_info['val_class_distribution']['class_1']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd1beab",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Definir Arquitetura MLP 1\n",
    "\n",
    "**Estrutura:**\n",
    "```\n",
    "Input (15) ‚Üí Dense(16, ReLU) + Dropout(0.2) + L2(0.01) ‚Üí Output(1, Sigmoid)\n",
    "```\n",
    "\n",
    "**Contagem de par√¢metros:**\n",
    "- Camada 1: 15 √ó 16 + 16 (bias) = **256 par√¢metros**\n",
    "- Camada 2: 16 √ó 1 + 1 (bias) = **17 par√¢metros**\n",
    "- **TOTAL: 273 par√¢metros**\n",
    "\n",
    "**Rela√ß√£o:** 712 amostras / 273 params = **2.61 amostras por par√¢metro** ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996e3b35",
   "metadata": {},
   "source": [
    "## üéõÔ∏è HIPERPAR√ÇMETROS EXPLICADOS\n",
    "\n",
    "Antes de implementar, vamos entender **O QUE** cada hiperpar√¢metro faz e **POR QU√ä** escolhemos esses valores.\n",
    "\n",
    "---\n",
    "\n",
    "### 1Ô∏è‚É£ LEARNING RATE = 0.001\n",
    "\n",
    "**O que √©:** Tamanho do \"passo\" ao ajustar os pesos durante o treino.\n",
    "\n",
    "**Como funciona:**\n",
    "```\n",
    "novo_peso = peso_antigo - (learning_rate √ó gradiente)\n",
    "```\n",
    "0,01 rapido demais - 0,001 correto. -  0,0001 mto lento, demora muito para convergir. \n",
    "\n",
    "**Por que 0.001?**\n",
    "- ‚úÖ Padr√£o comprovado para Adam optimizer\n",
    "- ‚úÖ R√°pido o suficiente (converge em ~30-50 epochs)\n",
    "- ‚úÖ Est√°vel (n√£o fica \"pulando\")\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ BATCH SIZE = 32\n",
    "\n",
    "**O que √©:** Quantas amostras processar antes de atualizar os pesos.\n",
    "\n",
    "**Compara√ß√£o:**\n",
    "\n",
    "```\n",
    "BATCH = 1 (online):          BATCH = 32 (mini-batch):     BATCH = 712 (full):\n",
    "Processa 1 passageiro  ‚Üí     Processa 32 passageiros ‚Üí    Processa TODOS ‚Üí\n",
    "Atualiza pesos         ‚Üí     Atualiza pesos          ‚Üí    Atualiza pesos\n",
    "Processa pr√≥ximo       ‚Üí     Processa pr√≥ximos 32    ‚Üí    (1 atualiza√ß√£o\n",
    "(712 atualiza√ß√µes!)          (22 atualiza√ß√µes)            por √©poca!)\n",
    "\n",
    "‚úÖ R√°pido mas barulhento    ‚úÖ Equil√≠brio perfeito!      ‚ö†Ô∏è Lento, pode travar\n",
    "```\n",
    "\n",
    "**Por que 32?**\n",
    "- ‚úÖ Aprende padr√µes gerais (m√©dia de 32 amostras)\n",
    "- ‚úÖ Ainda r√°pido (22 updates por √©poca)\n",
    "- ‚úÖ GPU-friendly (se usar acelera√ß√£o)\n",
    "\n",
    "**Analogia:** √â como estudar um livro:\n",
    "- Batch=1: Aprender ap√≥s cada palavra (barulhento)\n",
    "- Batch=32: Aprender ap√≥s cada par√°grafo (ideal)\n",
    "- Batch=712: Ler o livro inteiro para depois estudar (lento)\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ DROPOUT = 0.2\n",
    "\n",
    "**O que √©:** Desliga 20% dos neur√¥nios aleatoriamente durante o treino.\n",
    "\n",
    "**Visualiza√ß√£o:**\n",
    "\n",
    "```\n",
    "SEM DROPOUT:                   COM DROPOUT (treino):\n",
    "Todas as conex√µes ativas       20% das conex√µes desligadas\n",
    "\n",
    "N1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí                     N1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí\n",
    "N2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí                     N2  ‚úó‚úó‚úó‚úó‚úó  (desligado)\n",
    "N3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí   OUTPUT            N3 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí   OUTPUT\n",
    "N4 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí                     N4 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí\n",
    "N5 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí                     N5  ‚úó‚úó‚úó‚úó‚úó  (desligado)\n",
    "...                            ...\n",
    "\n",
    "Rede decora padr√µes            Rede aprende padr√µes\n",
    "espec√≠ficos (overfit!)         robustos!\n",
    "```\n",
    "\n",
    "**Por que 0.2?**\n",
    "- ‚úÖ For√ßa a rede a aprender padr√µes distribu√≠dos\n",
    "- ‚úÖ Cada neur√¥nio n√£o pode \"depender\" de outro espec√≠fico\n",
    "- ‚úÖ 20% = equil√≠brio (n√£o muito, n√£o pouco)\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANTE:** Dropout s√≥ funciona no **treino**! Na **predi√ß√£o** todos os neur√¥nios s√£o usados.\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ L2 REGULARIZATION (alpha = 0.01)\n",
    "\n",
    "**O que √©:** Penalidade por ter pesos grandes.\n",
    "\n",
    "**F√≥rmula do Loss:**\n",
    "```\n",
    "Loss_total = Loss_erro + (0.01 √ó soma_dos_pesos¬≤)\n",
    "```\n",
    "\n",
    "**Impacto visual:**\n",
    "\n",
    "```\n",
    "SEM L2:                        COM L2:\n",
    "Peso_Age = 500.0              Peso_Age = 2.5\n",
    "Peso_Sex = 300.0              Peso_Sex = 8.0\n",
    "\n",
    "Rede decora:                   Rede generaliza:\n",
    "\"Se Age=0.22 EXATAMENTE,       \"Idade influencia\n",
    " ent√£o sobrevive\"              moderadamente\"\n",
    "\n",
    "Overfitting! ‚úó                 Generaliza√ß√£o! ‚úì\n",
    "```\n",
    "\n",
    "**Por que 0.01?**\n",
    "- ‚úÖ Mant√©m pesos pequenos\n",
    "- ‚úÖ Evita overfitting\n",
    "- ‚úÖ N√£o √© t√£o forte que impe√ßa aprendizado\n",
    "\n",
    "**Analogia:** √â como uma taxa de impostos sobre pesos grandes. A rede tenta \"economizar\" mantendo pesos pequenos.\n",
    "\n",
    "---\n",
    "\n",
    "### 5Ô∏è‚É£ ACTIVATION = 'relu' (Rectified Linear Unit)\n",
    "\n",
    "**O que √©:** Fun√ß√£o aplicada ap√≥s a soma ponderada.\n",
    "\n",
    "**F√≥rmula:**\n",
    "```\n",
    "ReLU(x) = max(0, x)\n",
    "\n",
    "Se x = -2.5  ‚Üí  ReLU = 0     (bloqueia)\n",
    "Se x = 0.0   ‚Üí  ReLU = 0     (bloqueia)\n",
    "Se x = 3.7   ‚Üí  ReLU = 3.7   (passa!)\n",
    "```\n",
    "\n",
    "**Por que ReLU?**\n",
    "- ‚úÖ Introduz n√£o-linearidade (permite aprender padr√µes complexos)\n",
    "- ‚úÖ Simples e r√°pido de calcular\n",
    "- ‚úÖ Evita \"vanishing gradient\" (problema de outras ativa√ß√µes)\n",
    "\n",
    "**Exemplo pr√°tico:**\n",
    "```\n",
    "Neur√¥nio recebe: Age=0.22, Sex=1, Fare=0.14\n",
    "Soma ponderada: (0.22√ów1) + (1√ów2) + (0.14√ów3) + bias = 2.5\n",
    "ReLU(2.5) = 2.5  ‚úì Neur√¥nio ATIVA (detectou padr√£o!)\n",
    "\n",
    "Outro passageiro: Age=0.5, Sex=0, Fare=0.01\n",
    "Soma: (0.5√ów1) + (0√ów2) + (0.01√ów3) + bias = -1.2\n",
    "ReLU(-1.2) = 0  ‚úó Neur√¥nio DESLIGA (n√£o viu padr√£o relevante)\n",
    "```\n",
    "\n",
    "**Sem ReLU:** Rede seria apenas uma equa√ß√£o linear (in√∫til para padr√µes complexos!)\n",
    "\n",
    "---\n",
    "\n",
    "### 6Ô∏è‚É£ EARLY STOPPING (patience = 20)\n",
    "\n",
    "**O que √©:** Para o treino se valida√ß√£o n√£o melhorar por 20 √©pocas.\n",
    "\n",
    "**Timeline visual:**\n",
    "\n",
    "```\n",
    "√âPOCA 1:  val_loss = 0.55  [MELHOR! ‚≠ê Salva modelo]\n",
    "√âPOCA 2:  val_loss = 0.52  [MELHOR! ‚≠ê Salva modelo]\n",
    "√âPOCA 3:  val_loss = 0.50  [MELHOR! ‚≠ê Salva modelo]\n",
    "√âPOCA 4:  val_loss = 0.48  [MELHOR! ‚≠ê Salva modelo]\n",
    "√âPOCA 5:  val_loss = 0.49  [Piorou... contador = 1]\n",
    "√âPOCA 6:  val_loss = 0.50  [Piorou... contador = 2]\n",
    "...\n",
    "√âPOCA 24: val_loss = 0.51  [contador = 20 ‚Üí PARA!]\n",
    "\n",
    "Retorna ao modelo da √âPOCA 4 (melhor val_loss = 0.48)\n",
    "```\n",
    "\n",
    "**Por que usar?**\n",
    "- ‚úÖ **Evita overfitting:** Para antes de come√ßar a decorar\n",
    "- ‚úÖ **Economiza tempo:** N√£o treina 200 √©pocas se j√° convergiu\n",
    "- ‚úÖ **Pega o melhor:** Salva pesos da melhor √©poca automaticamente\n",
    "\n",
    "---\n",
    "\n",
    "### 7Ô∏è‚É£ ADAM OPTIMIZER\n",
    "\n",
    "**O que √©:** Algoritmo que ajusta os pesos durante treino.\n",
    "\n",
    "**Como funciona:**\n",
    "1. Calcula o erro (quanto errou)\n",
    "2. Calcula gradiente (dire√ß√£o para reduzir erro)\n",
    "3. **Adapta** o learning rate automaticamente por par√¢metro\n",
    "4. Atualiza pesos na dire√ß√£o correta\n",
    "\n",
    "**Compara√ß√£o:**\n",
    "\n",
    "```\n",
    "SGD (b√°sico):              ADAM:\n",
    "Passo fixo sempre          Ajusta passo automaticamente\n",
    "‚ö´ ‚Üí ‚ö´ ‚Üí ‚ö´ ‚Üí ‚ö´ ‚Üí ‚ö´        ‚ö´ ‚îÄ‚îÄ‚îÄ‚Üí ‚ö´ ‚îÄ‚îÄ‚Üí ‚ö´ ‚úì\n",
    "Lento                      R√°pido e eficiente\n",
    "```\n",
    "\n",
    "**Por que Adam?**\n",
    "- ‚úÖ Converge mais r√°pido que SGD\n",
    "- ‚úÖ Adapta learning rate sozinho\n",
    "- ‚úÖ Funciona bem na maioria dos problemas\n",
    "\n",
    "---\n",
    "\n",
    "**Resumo visual dos hiperpar√¢metros:**\n",
    "\n",
    "| Hiperpar√¢metro | Valor | Impacto |\n",
    "|----------------|-------|---------|\n",
    "| Learning Rate | 0.001 | Velocidade de aprendizado |\n",
    "| Batch Size | 32 | Balan√ßo estabilidade/velocidade |\n",
    "| Dropout | 0.2 | Anti-overfitting |\n",
    "| L2 (alpha) | 0.01 | Mant√©m pesos pequenos |\n",
    "| ReLU | - | Permite n√£o-linearidade |\n",
    "| Early Stop | 20 √©pocas | Para se n√£o melhorar |\n",
    "| Adam | - | Otimizador adaptativo |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbf779a",
   "metadata": {},
   "source": [
    "## üéØ RANDOM STATE = 42\n",
    "\n",
    "**O que √©:** Semente para gerar n√∫meros aleat√≥rios de forma **reproduz√≠vel**.\n",
    "\n",
    "**O problema sem random state:**\n",
    "\n",
    "```\n",
    "EXECU√á√ÉO 1:                    EXECU√á√ÉO 2:\n",
    "Dropout desliga neur√¥nios:     Dropout desliga neur√¥nios:\n",
    "N2, N5, N8                     N1, N4, N7\n",
    "\n",
    "Pesos iniciais:                Pesos iniciais:\n",
    "w1 = 0.23                      w1 = -0.15\n",
    "w2 = -0.41                     w2 = 0.67\n",
    "...                            ...\n",
    "\n",
    "Resultado:                     Resultado:\n",
    "ROC-AUC = 0.8512               ROC-AUC = 0.8298\n",
    "\n",
    "‚ùå N√£o d√° pra reproduzir!\n",
    "```\n",
    "\n",
    "**Com random state = 42:**\n",
    "\n",
    "```\n",
    "EXECU√á√ÉO 1:                    EXECU√á√ÉO 2:\n",
    "np.random.seed(42)             np.random.seed(42)\n",
    "random.seed(42)                random.seed(42)\n",
    "tf.random.set_seed(42)         tf.random.set_seed(42)\n",
    "\n",
    "Dropout: N2, N5, N8            Dropout: N2, N5, N8 ‚úì\n",
    "Pesos: w1=0.23, w2=-0.41       Pesos: w1=0.23, w2=-0.41 ‚úì\n",
    "Resultado: 0.8512              Resultado: 0.8512 ‚úì\n",
    "\n",
    "‚úÖ ID√äNTICO! Experimento reproduz√≠vel!\n",
    "```\n",
    "\n",
    "**Onde o random state √© usado:**\n",
    "1. **Divis√£o train/val:** Quais passageiros v√£o para treino/valida√ß√£o\n",
    "2. **Inicializa√ß√£o dos pesos:** Valores iniciais dos pesos\n",
    "3. **Dropout:** Quais neur√¥nios desligar em cada itera√ß√£o\n",
    "4. **Batch shuffling:** Ordem de processamento dos dados\n",
    "\n",
    "**Por que 42?**\n",
    "- Conven√ß√£o da comunidade (refer√™ncia \"Guia do Mochileiro das Gal√°xias\")\n",
    "- Qualquer n√∫mero serve, desde que seja **sempre o mesmo**!\n",
    "\n",
    "**Import√¢ncia cient√≠fica:**\n",
    "- ‚úÖ Outros pesquisadores podem reproduzir seu experimento\n",
    "- ‚úÖ Voc√™ consegue debugar (mesmos resultados toda vez)\n",
    "- ‚úÖ Compara√ß√µes justas entre arquiteturas\n",
    "\n",
    "---\n",
    "\n",
    "**Agora que entendemos TODOS os conceitos, vamos √† implementa√ß√£o! üöÄ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4c361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_mlp1(input_dim=15, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    MLP 1 - Minimalista\n",
    "    Input(15) ‚Üí Dense(16, ReLU) ‚Üí Output(1, Sigmoid)\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(16, activation='relu', \n",
    "              input_dim=input_dim,\n",
    "              kernel_regularizer=l2(0.01),\n",
    "              name='hidden_layer'),\n",
    "        Dropout(0.2, name='dropout'),\n",
    "        Dense(1, activation='sigmoid', name='output_layer')\n",
    "    ])\n",
    "    \n",
    "    # Compilar modelo\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[AUC(name='auc'), \n",
    "                 keras.metrics.Precision(name='precision'),\n",
    "                 keras.metrics.Recall(name='recall')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Criar modelo\n",
    "model = criar_mlp1(input_dim=X_train.shape[1])\n",
    "\n",
    "# Resumo da arquitetura\n",
    "print(\"=\"*70)\n",
    "print(\"üèóÔ∏è ARQUITETURA MLP 1 - MINIMALISTA\")\n",
    "print(\"=\"*70)\n",
    "model.summary()\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525457aa",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configurar Hiperpar√¢metros e Callbacks\n",
    "\n",
    "**Hiperpar√¢metros escolhidos:**\n",
    "- **Learning Rate:** 0.001 (padr√£o Adam, funciona bem para maioria dos casos)\n",
    "- **Batch Size:** 32 (equil√≠brio entre estabilidade e velocidade)\n",
    "- **Epochs:** 200 (com Early Stopping para evitar overfitting)\n",
    "\n",
    "**Callbacks:**\n",
    "- **EarlyStopping:** Para se val_loss n√£o melhorar por 20 epochs\n",
    "- **ModelCheckpoint:** Salva melhor modelo (menor val_loss)\n",
    "- **ReduceLROnPlateau:** Reduz learning rate se val_loss estagnar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a884d200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperpar√¢metros\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath='../artifacts/mlp1_best_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Hiperpar√¢metros configurados:\")\n",
    "print(f\"   - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   - Epochs: {EPOCHS}\")\n",
    "print(f\"   - Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"   - Callbacks: EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1e1899",
   "metadata": {},
   "source": [
    "## üöÄ Treinar Modelo\n",
    "\n",
    "Iniciamos o treinamento com valida√ß√£o cont√≠nua para monitorar overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027e060",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Iniciando treinamento...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ Treinamento completo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f204edc6",
   "metadata": {},
   "source": [
    "## üìä Visualizar Curvas de Aprendizado\n",
    "\n",
    "Analisamos Loss e ROC-AUC para detectar overfitting/underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e61cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('MLP 1 - Loss por Epoch', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# ROC-AUC\n",
    "axes[1].plot(history.history['auc'], label='Train AUC', linewidth=2)\n",
    "axes[1].plot(history.history['val_auc'], label='Val AUC', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('ROC-AUC', fontsize=12)\n",
    "axes[1].set_title('MLP 1 - ROC-AUC por Epoch', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/mlp1_learning_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Gr√°ficos salvos em: reports/figures/mlp1_learning_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e834a",
   "metadata": {},
   "source": [
    "## üéØ Avaliar Performance no Conjunto de Valida√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e5b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predi√ß√µes\n",
    "y_pred_proba = model.predict(X_val)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).ravel()\n",
    "\n",
    "# M√©tricas\n",
    "val_roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä M√âTRICAS DE VALIDA√á√ÉO - MLP 1\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüéØ ROC-AUC Score: {val_roc_auc:.4f}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã Classification Report:\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(y_val, y_pred, target_names=['Morreu (0)', 'Sobreviveu (1)']))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b386a9",
   "metadata": {},
   "source": [
    "## üîç Matriz de Confus√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b07b81",
   "metadata": {},
   "source": [
    "## üìà Curva ROC (Receiver Operating Characteristic)\n",
    "\n",
    "A curva ROC mostra o trade-off entre Taxa de Verdadeiros Positivos (sensibilidade) e Taxa de Falsos Positivos para diferentes thresholds de decis√£o. Quanto maior a √°rea sob a curva (AUC), melhor o modelo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7020cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular curva ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plotar curva ROC\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance (AUC = 0.50)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taxa de Falsos Positivos (FPR)', fontsize=12)\n",
    "plt.ylabel('Taxa de Verdadeiros Positivos (TPR)', fontsize=12)\n",
    "plt.title('MLP 1 - Curva ROC (Valida√ß√£o)', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/mlp1_roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Curva ROC salva em: reports/figures/mlp1_roc_curve.png\")\n",
    "print(f\"üìä ROC-AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd40ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Morreu (0)', 'Sobreviveu (1)'],\n",
    "            yticklabels=['Morreu (0)', 'Sobreviveu (1)'],\n",
    "            cbar_kws={'label': 'Quantidade'})\n",
    "plt.xlabel('Predi√ß√£o', fontsize=12)\n",
    "plt.ylabel('Real', fontsize=12)\n",
    "plt.title('MLP 1 - Matriz de Confus√£o (Valida√ß√£o)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/mlp1_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Matriz de confus√£o salva em: reports/figures/mlp1_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63366eca",
   "metadata": {},
   "source": [
    "## üíæ Salvar M√©tricas e Modelo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7adb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar hist√≥rico de treino\n",
    "history_dict = {\n",
    "    'loss': [float(x) for x in history.history['loss']],\n",
    "    'val_loss': [float(x) for x in history.history['val_loss']],\n",
    "    'auc': [float(x) for x in history.history['auc']],\n",
    "    'val_auc': [float(x) for x in history.history['val_auc']],\n",
    "    'precision': [float(x) for x in history.history['precision']],\n",
    "    'val_precision': [float(x) for x in history.history['val_precision']],\n",
    "    'recall': [float(x) for x in history.history['recall']],\n",
    "    'val_recall': [float(x) for x in history.history['val_recall']]\n",
    "}\n",
    "\n",
    "with open('../reports/mlp1_history.json', 'w') as f:\n",
    "    json.dump(history_dict, f, indent=4)\n",
    "\n",
    "# Salvar m√©tricas finais\n",
    "metrics = {\n",
    "    'model_name': 'MLP1_Minimalista',\n",
    "    'architecture': 'Input(15) -> Dense(16, ReLU) -> Output(1, Sigmoid)',\n",
    "    'total_params': 273,\n",
    "    'trainable_params': 273,\n",
    "    'hyperparameters': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'epochs_run': len(history.history['loss']),\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'dropout': 0.2,\n",
    "        'l2_regularization': 0.01\n",
    "    },\n",
    "    'val_metrics': {\n",
    "        'roc_auc': float(val_roc_auc),\n",
    "        'final_val_loss': float(history.history['val_loss'][-1]),\n",
    "        'best_val_loss': float(min(history.history['val_loss'])),\n",
    "        'best_val_auc': float(max(history.history['val_auc']))\n",
    "    },\n",
    "    'confusion_matrix': cm.tolist()\n",
    "}\n",
    "\n",
    "with open('../reports/mlp1_metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "\n",
    "# Salvar modelo final\n",
    "model.save('../artifacts/mlp1_final_model.keras')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üíæ ARQUIVOS SALVOS\")\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ Modelo: artifacts/mlp1_best_model.keras (melhor val_loss)\")\n",
    "print(\"‚úÖ Modelo: artifacts/mlp1_final_model.keras (final)\")\n",
    "print(\"‚úÖ Hist√≥rico: reports/mlp1_history.json\")\n",
    "print(\"‚úÖ M√©tricas: reports/mlp1_metrics.json\")\n",
    "print(\"‚úÖ Gr√°ficos: reports/figures/mlp1_*.png\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016aff28",
   "metadata": {},
   "source": [
    "## üéâ RESUMO FINAL - MLP 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d28c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ SEMANA 2 COMPLETA - MLP 1 MINIMALISTA\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "‚úÖ Arquitetura implementada: Input(15) ‚Üí Dense(16) ‚Üí Output(1)\n",
    "‚úÖ Total de par√¢metros: 273\n",
    "‚úÖ Rela√ß√£o amostras/par√¢metros: {712/273:.2f}\n",
    "\n",
    "üìä RESULTADOS:\n",
    "   - ROC-AUC Valida√ß√£o: {val_roc_auc:.4f}\n",
    "   - Melhor Val Loss: {min(history.history['val_loss']):.4f}\n",
    "   - Melhor Val AUC: {max(history.history['val_auc']):.4f}\n",
    "   - Epochs executados: {len(history.history['loss'])}\n",
    "\n",
    "üìà AN√ÅLISE:\n",
    "   - Baseline estabelecido com sucesso\n",
    "   - Modelo simples e interpret√°vel\n",
    "   - Baixo risco de overfitting confirmado\n",
    "\n",
    "üéØ PR√ìXIMOS PASSOS:\n",
    "   - Semana 3: Implementar MLP 2 (Moderada) ou Ensemble 3\n",
    "   - Comparar performance entre arquiteturas\n",
    "   - Decidir melhor modelo para produ√ß√£o\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
